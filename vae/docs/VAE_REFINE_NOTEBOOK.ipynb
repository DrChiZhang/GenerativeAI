{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed98cc0",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE) Refinement - Session Notes\n",
    "\n",
    "## 1. Initial Code, Problems, and First Refinements\n",
    "- Started with a VAE implementation in PyTorch.\n",
    "- Identified issues:\n",
    "    - Hardcoded flattening/reshaping sizes in encoder/decoder.\n",
    "    - No flexibility for loss type or activation function.\n",
    "    - Some aspects of documentation and maintainability could improve.\n",
    "\n",
    "### First Refinements Highlights\n",
    "- Used utility methods for encoder/decoder construction.\n",
    "- Consistently type-annotated and documented all methods.\n",
    "- Provided flexibility in model design and loss computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe2e1e",
   "metadata": {},
   "source": [
    "## 2. Further Refinement Points\n",
    "- Dynamically calculate and use encoder output shape in flatten/reshape.\n",
    "- Parameterize activation function and loss type.\n",
    "- Avoid overuse of `nn.Sequential`.\n",
    "- Provide comprehensive input shape/type checking.\n",
    "- Improve testing practices for model flexibility and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Tuple, Callable, Any, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VAEOutput:\n",
    "    loss: torch.Tensor\n",
    "    recon_loss: torch.Tensor\n",
    "    kld: torch.Tensor\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_dim: int,\n",
    "        input_shape: Tuple[int, int, int],   # (C, H, W)\n",
    "        hidden_dims: Optional[List[int]] = None,\n",
    "        latent_dim: int = 16,\n",
    "        activation: Optional[Callable[[], nn.Module]] = None,\n",
    "        recon_loss_type: str = \"mse\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(input_shape, (tuple, list)) and len(input_shape) == 3,             \"input_shape must be a tuple (channels, H, W)\"\n",
    "        self.x_dim = x_dim\n",
    "        self.recon_loss_type = recon_loss_type.lower()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        if activation is None:\n",
    "            self.act = nn.LeakyReLU(0.2)\n",
    "        else:\n",
    "            self.act = activation()\n",
    "\n",
    "        # Encoder\n",
    "        modules = []\n",
    "        in_channels = x_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(nn.Conv2d(in_channels, h_dim, kernel_size=3, stride=2, padding=1))\n",
    "            modules.append(nn.BatchNorm2d(h_dim))\n",
    "            modules.append(self.act)\n",
    "            in_channels = h_dim\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "        # Compute encoded output shape\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *input_shape)\n",
    "            enc_out = self.encoder(dummy)\n",
    "            self.enc_out_shape = enc_out.shape[1:]  # (C, H, W)\n",
    "            self.enc_flat_dim = enc_out.numel() // enc_out.shape[0]\n",
    "\n",
    "        self.fc_mu = nn.Linear(self.enc_flat_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.enc_flat_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, self.enc_flat_dim)\n",
    "        decoder_modules = []\n",
    "        reversed_hidden_dims = list(reversed(hidden_dims))\n",
    "        in_channels = self.enc_out_shape[0]\n",
    "        for i in range(len(reversed_hidden_dims) - 1):\n",
    "            decoder_modules.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels, reversed_hidden_dims[i + 1],\n",
    "                    kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "                )\n",
    "            )\n",
    "            decoder_modules.append(nn.BatchNorm2d(reversed_hidden_dims[i + 1]))\n",
    "            decoder_modules.append(self.act)\n",
    "            in_channels = reversed_hidden_dims[i + 1]\n",
    "\n",
    "        self.decoder = nn.Sequential(*decoder_modules)\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, x_dim, kernel_size=3, stride=2,\n",
    "                padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        result = self.encoder(x)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, *self.enc_out_shape)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, x, mu, log_var\n",
    "\n",
    "    def loss_function(\n",
    "        self, recons: torch.Tensor, input: torch.Tensor,\n",
    "        mu: torch.Tensor, log_var: torch.Tensor, kld_weight: float = 1.0\n",
    "    ) -> VAEOutput:\n",
    "        if self.recon_loss_type == 'bce':\n",
    "            recon_loss = F.binary_cross_entropy(recons, input, reduction='mean')\n",
    "        else:\n",
    "            recon_loss = F.mse_loss(recons, input)\n",
    "\n",
    "        kld = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1).mean()\n",
    "        loss = recon_loss + kld_weight * kld\n",
    "        return VAEOutput(loss=loss, recon_loss=recon_loss.detach(), kld=kld.detach())\n",
    "\n",
    "    def sample(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        z = torch.randn(batch_size, self.fc_mu.out_features).to(device)\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(x)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43b21e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Example Test Suite for the Refined VAE\n",
    "\n",
    "- Shows how to use `unittest` to check initialization, forward pass, loss, and shape adaptiveness.\n",
    "- Demonstrates best practices for flexible PyTorch model testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b01bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import unittest\n",
    "\n",
    "class TestVAE(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.input_channels = 3\n",
    "        self.input_size = 64\n",
    "        self.latent_dim = 10\n",
    "        self.model = VAE(\n",
    "            x_dim=self.input_channels,\n",
    "            input_shape=(self.input_channels, self.input_size, self.input_size),\n",
    "            hidden_dims=[32, 64, 128],\n",
    "            latent_dim=self.latent_dim,\n",
    "        )\n",
    "\n",
    "    def test_vae_initialization(self):\n",
    "        self.assertIsInstance(self.model.encoder, torch.nn.Module)\n",
    "        self.assertIsInstance(self.model.decoder, torch.nn.Module)\n",
    "\n",
    "    def test_vae_forward(self):\n",
    "        batch = 8\n",
    "        H = W = self.input_size\n",
    "        x = torch.randn(batch, self.input_channels, H, W)\n",
    "        recon_x, inp_x, mu, log_var = self.model(x)\n",
    "        self.assertEqual(recon_x.shape, (batch, self.input_channels, H, W))\n",
    "        self.assertEqual(inp_x.shape, (batch, self.input_channels, H, W))\n",
    "        self.assertEqual(mu.shape, (batch, self.latent_dim))\n",
    "        self.assertEqual(log_var.shape, (batch, self.latent_dim))\n",
    "\n",
    "    def test_vae_loss(self):\n",
    "        batch = 4\n",
    "        x = torch.randn(batch, self.input_channels, self.input_size, self.input_size)\n",
    "        recon_x, inp_x, mu, log_var = self.model(x)\n",
    "        loss_out = self.model.loss_function(recon_x, inp_x, mu, log_var, kld_weight=0.01)\n",
    "        self.assertTrue(hasattr(loss_out, 'loss'))\n",
    "        self.assertTrue(loss_out.loss.requires_grad)\n",
    "        self.assertGreaterEqual(loss_out.loss.item(), 0)\n",
    "\n",
    "    def test_enc_dec_shapes(self):\n",
    "        # Test for different input shape (32x32)\n",
    "        model32 = VAE(\n",
    "            x_dim=3, input_shape=(3, 32, 32),\n",
    "            hidden_dims=[32, 64], latent_dim=5)\n",
    "        x = torch.randn(2, 3, 32, 32)\n",
    "        out, _, mu, log_var = model32(x)\n",
    "        self.assertEqual(out.shape, (2, 3, 32, 32))\n",
    "        self.assertEqual(mu.shape, (2, 5))\n",
    "        self.assertEqual(log_var.shape, (2, 5))\n",
    "\n",
    "# If running interactively, remove the following block or use: unittest.main(argv=[''], exit=False)\n",
    "# if __name__ == '__main__':\n",
    "#     unittest.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d200a0",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Summary\n",
    "\n",
    "- The refined VAE is modular, robust, and model-agnostic, supporting various channel, height, and width sizes.\n",
    "- Testing demonstrates flexibility for various inputs, shapes, and code safety.\n",
    "- Always provide `input_shape` as a tuple (C, H, W).\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [VAE Theory - Kingma & Welling 2014](https://arxiv.org/abs/1312.6114)\n",
    "- [PyTorch VAE Tutorial](https://pytorch.org/tutorials/beginner/torchvision_tutorial.html)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session Export**\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
