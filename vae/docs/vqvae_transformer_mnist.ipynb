{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE + Transformer Prior on MNIST\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Training a Vector Quantized VAE (VQ-VAE) on MNIST\n",
    "- Extracting quantized codebook indices as discrete image representations\n",
    "- Training a causal Transformer on the discrete token sequences (as a prior)\n",
    "- Autoregressively sampling new images using the Transformer + VQ-VAE decoder"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ----- Imports and setup -----\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQ-VAE Model\n",
    "- Discrete bottleneck enabled by a codebook (vector quantizer)\n",
    "- Shallow model for MNIST, but you can go deeper for complex data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class VAEOutput:\n",
    "    loss: torch.Tensor\n",
    "    recon_loss: torch.Tensor\n",
    "    vq_loss: torch.Tensor\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Discretizes latents from encoder, selects nearest codebook vector per spatial location.\n",
    "    Implements the VQ-VAE \"commitment\" and \"embedding\" loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, beta: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.K = num_embeddings\n",
    "        self.D = embedding_dim\n",
    "        self.beta = beta\n",
    "        self.embedding = nn.Embedding(self.K, self.D)\n",
    "        self.embedding.weight.data.uniform_(-1 / self.K, 1 / self.K)\n",
    "    def forward(self, latents: torch.Tensor):\n",
    "        # Reorder to (B, H, W, D)\n",
    "        latents = latents.permute(0,2,3,1).contiguous()\n",
    "        shape = latents.shape\n",
    "        # Flatten batch and spatial dims, shape (BHW, D)\n",
    "        flat_latents = latents.view(-1, self.D)\n",
    "        # Compute squared distances to codebook\n",
    "        dist = torch.sum(flat_latents**2,dim=1,keepdim=True) \\\n",
    "               + torch.sum(self.embedding.weight**2,dim=1) \\\n",
    "               - 2*torch.matmul(flat_latents, self.embedding.weight.t())\n",
    "        # For each code, get index of nearest codebook vector\n",
    "        encoding_inds = torch.argmin(dist,dim=1,keepdim=True)\n",
    "        one_hot = torch.zeros(encoding_inds.size(0), self.K, device=latents.device)\n",
    "        one_hot.scatter_(1, encoding_inds, 1)\n",
    "        quantized = torch.matmul(one_hot, self.embedding.weight).view(shape)\n",
    "        # Losses\n",
    "        commitment_loss = F.mse_loss(quantized.detach(), latents)\n",
    "        embedding_loss = F.mse_loss(quantized, latents.detach())\n",
    "        vq_loss = self.beta * commitment_loss + embedding_loss\n",
    "        # Pass gradients through quantized using 'straight-through estimator'\n",
    "        quantized = latents + (quantized - latents).detach()\n",
    "        return quantized.permute(0,3,1,2).contiguous(), vq_loss\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"Standard residual block as used in VQ-VAE.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.resblock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "        )\n",
    "    def forward(self, x): return x + self.resblock(x)\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    \"\"\"VQ-VAE Encoder + Quantizer + Decoder.\"\"\"\n",
    "    def __init__(self, in_channels, embedding_dim, num_embeddings, hidden_dims=None, beta=0.25):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        if hidden_dims is None: hidden_dims=[64,128]\n",
    "        # Encoder: image to \"latent image\"\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, h_dim, 4, 2, 1),\n",
    "                    nn.LeakyReLU()\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "        for _ in range(2):\n",
    "            modules.append(ResidualLayer(in_channels, in_channels))\n",
    "        modules.append(nn.Conv2d(in_channels, embedding_dim, 1))\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.vq = VectorQuantizer(num_embeddings, embedding_dim, beta)\n",
    "        dec_mod = []\n",
    "        dec_mod.append(nn.Conv2d(embedding_dim, hidden_dims[-1], 3, 1, 1))\n",
    "        for _ in range(2):\n",
    "            dec_mod.append(ResidualLayer(hidden_dims[-1], hidden_dims[-1]))\n",
    "        dec_mod.append(nn.ConvTranspose2d(hidden_dims[-1], hidden_dims[0], 4, 2, 1))\n",
    "        dec_mod.append(nn.LeakyReLU())\n",
    "        dec_mod.append(nn.ConvTranspose2d(hidden_dims[0], 1, 4, 2, 1))\n",
    "        dec_mod.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*dec_mod)\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        q, vq_loss = self.vq(z)\n",
    "        out = self.decoder(q)\n",
    "        return out, x, vq_loss\n",
    "    def loss_function(self, *args):\n",
    "        recons, input, vq_loss = args\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "        loss = recons_loss + vq_loss\n",
    "        return VAEOutput(loss, recons_loss.detach(), vq_loss.detach())\n",
    "    def get_codebook_indices(self, x):\n",
    "        z = self.encoder(x)\n",
    "        latents = z.permute(0,2,3,1).contiguous()\n",
    "        flat_latents = latents.view(-1, self.vq.D)\n",
    "        dist = (\n",
    "            torch.sum(flat_latents**2, dim=1,keepdim=True)\n",
    "            + torch.sum(self.vq.embedding.weight**2, dim=1)\n",
    "            - 2*torch.matmul(flat_latents, self.vq.embedding.weight.t())\n",
    "        )\n",
    "        inds = torch.argmin(dist,dim=1)\n",
    "        N, H, W, D = latents.shape\n",
    "        return inds.view(N, H, W)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MNIST data (binarized 28x28 images, batches of size 64) "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trainset = torchvision.datasets.MNIST('./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "loader = DataLoader(trainset, batch_size=64, shuffle=True)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VQ-VAE on MNIST\n",
    "- Downsamples images to a latent grid\n",
    "- Optimizes for MSE + VQ loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vqvae = VQVAE(1, 8, 64, [64,128], beta=0.25).to(device)\n",
    "vqvae.train()\n",
    "opt = torch.optim.Adam(vqvae.parameters(), lr=2e-3)\n",
    "\n",
    "for epoch in range(4):\n",
    "    for img, _ in loader:\n",
    "        img = img.to(device)\n",
    "        out = vqvae(img)\n",
    "        vo = vqvae.loss_function(*out)\n",
    "        opt.zero_grad()\n",
    "        vo.loss.backward()\n",
    "        opt.step()\n",
    "    print(f'VQ-VAE Epoch {epoch+1} Loss: {vo.loss.item():.4f}')"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract codebook indices for each image\n",
    "- Each image is now represented as a `(H, W)` grid of discrete integer codes.\n",
    "- We'll use these as tokens for the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vqvae.eval()\n",
    "all_codes = []\n",
    "with torch.no_grad():\n",
    "    for img, _ in loader:\n",
    "        img = img.to(device)\n",
    "        code = vqvae.get_codebook_indices(img).cpu()\n",
    "        all_codes.append(code)\n",
    "all_codes = torch.cat(all_codes, dim=0) # (N, H, W)\n",
    "flat_codes = all_codes.view(all_codes.size(0), -1) # (N, seq_len)\n",
    "seq_len = flat_codes.size(1)\n",
    "print(\"Flattened code sequence shape:\", flat_codes.shape)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal causal Transformer prior for discrete codes\n",
    "- Each code sequence is used as a 1D sequence of tokens.\n",
    "- Standard causal (autoregressive) mask is used so position t only attends to 0..t."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TokenTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, seq_len, d_model=128, nhead=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.seq_len = seq_len\n",
    "        self.token_emb = nn.Embedding(num_tokens, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, seq_len, d_model))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead), num_layers)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, num_tokens)\n",
    "    def forward(self, x):  # x: (batch, seq_len)\n",
    "        emb = self.token_emb(x) + self.pos_emb\n",
    "        emb = emb.permute(1,0,2)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(self.seq_len).to(x.device)\n",
    "        h = self.transformer(emb, mask)\n",
    "        h = h.permute(1,0,2)\n",
    "        return self.fc(self.ln(h))  # (batch, seq_len, num_tokens)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Transformer prior\n",
    "- At each step, predicts the next code index given previous indices.\n",
    "- Loss is categorical cross-entropy.\n",
    "- Simple teacher-forcing: input is [0, ..., L-2], target is [1, ..., L-1] (shifted sequence)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_tokens = 64\n",
    "transformer = TokenTransformer(num_tokens, seq_len).to(device)\n",
    "optim_t = torch.optim.Adam(transformer.parameters(), lr=2e-4)\n",
    "batch_size = 16\n",
    "\n",
    "print(\"Training Transformer prior...\")\n",
    "for epoch in range(4):\n",
    "    perm = torch.randperm(flat_codes.size(0))\n",
    "    for i in range(0, flat_codes.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        batch = flat_codes[idx].to(device)\n",
    "        inp = batch[:, :-1]\n",
    "        tgt = batch[:, 1:]\n",
    "        logits = transformer(inp)\n",
    "        logits = logits[:, :-1, :]\n",
    "        loss = F.cross_entropy(logits.reshape(-1, num_tokens), tgt.reshape(-1))\n",
    "        optim_t.zero_grad()\n",
    "        loss.backward()\n",
    "        optim_t.step()\n",
    "    print(f'Transformer Epoch {epoch+1} Loss: {loss.item():.4f}')"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample a new code sequence from the Transformer and decode with VQ-VAE\n",
    "\n",
    "The Transformer autoregressively predicts code indices one by one. The generated sequence is then mapped to embeddings and decoded into an image."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Generating from Transformer prior...\")\n",
    "transformer.eval()\n",
    "with torch.no_grad():\n",
    "    # Start with all zeros\n",
    "    seq = torch.zeros(1, seq_len, dtype=torch.long, device=device)\n",
    "    for t in range(seq_len-1):\n",
    "        logits = transformer(seq[:, :t+1])\n",
    "        next_token = torch.multinomial(F.softmax(logits[0, t], -1), 1)\n",
    "        seq[0, t+1] = next_token\n",
    "\n",
    "    H = W = int(np.sqrt(seq_len))\n",
    "    sampled_grid = seq.view(1, H, W)\n",
    "\n",
    "# Convert indices back to embeddings and decode\n",
    "emb = vqvae.vq.embedding(sampled_grid.view(-1)).view(1, H, W, vqvae.vq.D)\n",
    "emb = emb.permute(0, 3, 1, 2).contiguous()\n",
    "img_gen = vqvae.decoder(emb).cpu().detach().squeeze().numpy()\n",
    "\n",
    "plt.title(\"VQ-VAE + Transformer Prior: Sampled Image\")\n",
    "plt.imshow(img_gen, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "execution_count": 8,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}